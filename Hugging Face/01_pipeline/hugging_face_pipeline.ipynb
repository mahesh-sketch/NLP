{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9a7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06823595",
   "metadata": {},
   "source": [
    "1️⃣ Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6adee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997534155845642}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love Pizza\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e55ba",
   "metadata": {},
   "source": [
    "2️⃣ Text Generation (using GPT-like models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9733a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In future, Ai will have access to more data and information about people.\\n\\nThe Ai project, which is set to start in April 2016,'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation',model=\"gpt2\")\n",
    "text = generator(\"In future, Ai will\",max_length=30,num_return_sequences=1)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6ef0a",
   "metadata": {},
   "source": [
    "3️⃣ Masked Language Modeling (Fill-in-the-blank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e98406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.2438535839319229, 'token': 2109, 'token_str': 'used', 'sequence': 'transformers are used for nlp.'}, {'score': 0.05939435586333275, 'token': 2328, 'token_str': 'built', 'sequence': 'transformers are built for nlp.'}, {'score': 0.052648842334747314, 'token': 2800, 'token_str': 'available', 'sequence': 'transformers are available for nlp.'}, {'score': 0.05073538422584534, 'token': 2081, 'token_str': 'made', 'sequence': 'transformers are made for nlp.'}, {'score': 0.0470687597990036, 'token': 2734, 'token_str': 'needed', 'sequence': 'transformers are needed for nlp.'}]\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = fill_mask(\"Transformers are [MASK] for NLP.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6708a1",
   "metadata": {},
   "source": [
    "4️⃣ Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94d751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ORG', 'score': 0.9958663, 'word': 'Hugging Face Inc', 'start': 0, 'end': 16}, {'entity_group': 'LOC', 'score': 0.9992396, 'word': 'New York City', 'start': 30, 'end': 43}]\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "result = ner(\"Hugging Face Inc. is based in New York City.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee481280",
   "metadata": {},
   "source": [
    "5️⃣ Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35545573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9679760932922363, 'start': 35, 'end': 48, 'answer': 'New York City'}\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\")\n",
    "result = qa(question=\"Where is Hugging Face based?\", context=\"Hugging Face is a company based in New York City.\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
