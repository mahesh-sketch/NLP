{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "631f79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b330b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#define the text\n",
    "text = \"I absolutely loved the new Batman movie! It's a must-watch. #Batman #MovieReview\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da62c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  ['I', 'absolutely', 'loved', 'the', 'new', 'Batman', 'movie', '!', 'It', \"'s\", 'a', 'must', '-', 'watch', '.', '#', 'Batman', '#', 'MovieReview']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text)\n",
    "\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"Token: \", spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b89af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing:  ['i', 'absolutely', 'loved', 'the', 'new', 'batman', 'movie', '!', 'it', \"'s\", 'a', 'must', '-', 'watch', '.', '#', 'batman', '#', 'moviereview']\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing\n",
    "spacy_lower = [token.lower_ for token in doc]\n",
    "print(\"Lowercasing: \",spacy_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b6bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword Removal:  ['absolutely', 'loved', 'new', 'Batman', 'movie', '!', '-', 'watch', '.', '#', 'Batman', '#', 'MovieReview']\n"
     ]
    }
   ],
   "source": [
    "# Stopword Removal\n",
    "spacy_no_stop = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Stopword Removal: \",spacy_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91eafe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematization:  ['I', 'absolutely', 'love', 'the', 'new', 'Batman', 'movie', '!', 'it', 'be', 'a', 'must', '-', 'watch', '.', '#', 'Batman', '#', 'MovieReview']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "spacy_lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Lematization: \",spacy_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77faad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'absolutely',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'new',\n",
       " 'Batman',\n",
       " 'movie',\n",
       " '!',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'must',\n",
       " '-',\n",
       " 'watch',\n",
       " '.',\n",
       " '#',\n",
       " 'Batman',\n",
       " '#',\n",
       " 'Movie',\n",
       " 'Review']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Customize Tokenization\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"MovieReview\",[\n",
    "    {ORTH:\"Movie\"},\n",
    "    {ORTH:\"Review\"}\n",
    "])\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c413db3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x20a516c2b10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x20a516c1c10>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x20a51e67290>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x20a52e04790>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x20a52cb2f90>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x20a51e67060>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline names\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e75f0c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  |  PRON\n",
      "absolutely  |  ADV\n",
      "loved  |  VERB\n",
      "the  |  DET\n",
      "new  |  ADJ\n",
      "Batman  |  PROPN\n",
      "movie  |  NOUN\n",
      "!  |  PUNCT\n",
      "It  |  PRON\n",
      "'s  |  AUX\n",
      "a  |  DET\n",
      "must  |  AUX\n",
      "-  |  PUNCT\n",
      "watch  |  VERB\n",
      ".  |  PUNCT\n",
      "#  |  SYM\n",
      "Batman  |  PROPN\n",
      "#  |  PROPN\n",
      "Movie  |  PROPN\n",
      "Review  |  PROPN\n"
     ]
    }
   ],
   "source": [
    "# print(spacy_tokens)\n",
    "\n",
    "# spacy_pos = [token.pos_ for token in doc]\n",
    "# print(\"Part of Speach: \",spacy_pos)\n",
    "\n",
    "for token in doc:\n",
    "    print(token,' | ', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "361bfa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batman  |  PRODUCT  |  Objects, vehicles, foods, etc. (not services)\n",
      "#  |  CARDINAL  |  Numerals that do not fall under another type\n"
     ]
    }
   ],
   "source": [
    "#Name Entity Recognition\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86090c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "nt | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "i | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "# Customize LEEMA\n",
    "\n",
    "arr = nlp.get_pipe('attribute_ruler')\n",
    "arr.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{'LEMMA': \"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go ? Brah, dont say no! i am exhausted\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
